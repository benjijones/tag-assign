<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<!-- Note: RELEASE_VERSION and DATE are set by ant -->

<head>
  <title>The Oracle NoSQL Database (Release 12cR1.3.0.5 Client) Change Log</title>
  <meta name="description"
        content="Oracle NoSQL Database: A Distributed Key-Value Store.">
  <meta name="keywords"
        content="distributed,database,key,value,nosql,bigdata,transaction,transactions,java">
</head>
<body bgcolor="white">

<h2 align="center">Oracle NoSQL Database Change Log
</h2>
<p align="center">Release 12cR1.3.0.5 Client</p>

This release of Oracle NoSQL Database adds several new features
including user/password authentication and network security, secondary
zones, and support for typed data and a tabular data model, which adds
the ability to define secondary indexes on fields in a table.  The
table interface adds a new client API to access tables, indexes, and
data types, along with CLI to manage these new constructs.
<h3><a name="Upgrade">Upgrade Requirements</a></h3>
Upgrading an existing store to release 3.0 requires that the store be
running with a 2.0 release.  If you want to use release 3.0 with a store
created prior to the 2.0 release, be sure to upgrade the store to a
2.0 release before upgrading it to the 3.0 release.  Once a store has
been upgraded to release 3.0, it cannot be downgraded to an earlier
release.
<p>
See the section
on <a href="http://docs.oracle.com/cd/NOSQL/html/AdminGuide/deploymentupdate.html">
Updating an Existing Oracle NoSQL Database Deployment</a> in the Admin
Guide.
<p>
Release 3.0 is compatible with Java SE 7 and later, and has
been tested and certified against Oracle JDK 7u51. We encourage you to
upgrade to the latest Java releases to take advantage of the latest bug 
fixes and performance improvements.
</p>
<p>
Attempting to use this release with a version of Java earlier than Java
7 will produce an error message similar to:
<blockquote>
<pre>
Exception in thread "main" java.lang.UnsupportedClassVersionError:
  oracle/kv/impl/util/KVStoreMain : Unsupported major.minor version 51.0
</pre>
</blockquote>
</p>
<!-- ============================================================ -->
<hr>
<h3 align="center"><u>Changes in 12cR1.3.0.5 Client</u></h3>
<h3><a name="Features">New Features</a></h3>
<ol>
<li>
  A new client interface has been added that includes a set of
  datatypes and a tabular data model using those types. The tabular
  data model is used to provide support for secondary indexes which
  are defined on fields in a  table.  The model is discussed in the
  <a
  href="http://docs.oracle.com/cd/NOSQL/html/GettingStartedGuideTables/index.html">
  Getting Started Guide for Tables</a>
  <p>
  Tables and indexes are defined using the administrative CLI and
  accessed via programmatic API.  The data CLI has been enhanced to
  perform operations on tables and indexes as well.  The API is
  documented in the <a href="http://docs.oracle.com/cd/NOSQL/html/javadoc/index.html">
  Oracle NoSQL Database javadoc</a>, and is primarily in  the
  <i>oracle.kv.table</i> package.
  <p>
  It is possible to define tables that overlay data created with NoSQL
  DB Release 2 if that data was created using a conforming Avro
  schema.  This overlay is required in order to create secondary
  indexes on conforming Release 2 data.
  <p>
  The existing key/value interface remains available.
</li><br>
<li>
It is now possible to define secondary indexes for records.  See the previous
changelog entry about tables.  Index entries for a given record have
transactional consistency with their corresponding primary records.
Index iteration operations are part of the new table API.  Index scan
operations allow applications to iterate over raw indexes in 3
ways -- forward order, reverse order, and unordered.  It is possible
to define exact match and range scans in indexes.  Indexes can be on
single fields or defined as composite indexes on multiple fields in a
table.
</li><br>
<li>
Support for username/password authentication and secure network communications
has been added.  Existing applications that do not require this feature are not
impacted except for a change to makebootconfig, which adds a new required
argument (<code>-store-security</code>). Users that wish to use the new
capabilities should be aware of the following areas of change:
<ul>
<p>
<li>
The makebootconfig utility has a new required
<code>-store-security</code> command-line argument, which is used to
enable security for a new KVStore deployment. The -store-security flag
is also used to enable a non-secure deployment, and is actually
required in that case (-store-security none).
</li><br>
<li>
A new securityconfig tool has been added, which provides the ability to:
<ul>
<li>Create security configurations</li>
<li>Create/modified password stores</li>
<li>Enable security on a pre-existing, non-secure KVStore</li>
<li>Perform security maintenance tasks</li>
</ul>
</li><br>
<li>
Additional methods on the KVStoreConfig class:
<ul>
<li><code>setSecurityProperties()</code></li>
<li><code>getSecurityProperties()</code></li>
</ul>
</li><br>
<li>
An additional method on the KVStoreFactory class supporting authentication:
<ul>
<li><code>KVStoreFactory.getStore(KVStoreConfig, LoginCredentials, ReauthenticateHandler)</code></li>
</ul>
<p>
This new method utilizes the new interfaces
<ul>
<li><code>LoginCredentials</code></li>
<li><code>ReauthenticateHandler</code></li>
</ul>
<p>
As well as the new class <code>PasswordCredentials</code>.
</li>
<li>
Additional methods on the KVStore interface:
<ul>
<li><code>login()</code></li>
<li><code>logout()</code></li>
</ul>
</li><br>
<li>
New exception classes:
<ul>
<li><code>UnauthorizedException</code></li>
<li><code>AuthenticationFailureException</code></li>
<li><code>AuthenticationRequiredException</code></li>
</ul>
</li>
</ul>
<p>
Users should also familiarize themselves with security property files, which
are required when using a KVStore command-line utility program against a secure
store, and which may also be useful when running an application against a secure
store.
<p>
This feature is described in much greater depth in the Oracle NoSQL Database Security Guide, as well as in the Administrators Guide and product Javadoc.
</li><br>
<li>
The administrative CLI has been modified to use new terminology to refer
to data centers.  Data centers are now called <i>zones</i>.  The new
terminology is meant to clarify that these node groupings may not always
coincide with physical data centers.  A zone is a collection of nodes
that have good network connectivity with each other and have some level
of physical separation from nodes in other zones.  That physical
separation may mean that different zones are located in different
physical data center buildings, but could also represent different
floors, rooms, pods, or racks, depending on the particular deployment.
<p>
Commands that contained the word "datacenter" have been deprecated, and
are replaced with commands using the word "zone". The previous commands
will continue to work in this release. New commands are:
<ul>
<li><code>plan deploy-zone</code>
<li><code>plan remove-zone</code>
<li><code>show zones</code>
</ul>
<p>
Command flags that specify a zone have been changed to <code>-zn</code>,
for a zone ID, and <code>-znname</code>, for a zone name. The
earlier <code>-dc</code> and <code>-dcname</code> flags have been
deprecated but will continue to work in this release.  In addition, zone
IDs can now be specified using the "zn" prefix, with the earlier "dc"
prefix still currently supported.
<p>
The administrative GUI has also been modified to use the new Zone
terminology. [#22878]
</li><br>
<li>
There are now two types of zones.  <i>Primary</i> zones contain
electable nodes, which can serve as masters or replicas, and vote in
master elections.  All zones (or data centers) created in earlier
releases are primary zones, and new zones are created as primary zones
by default.  <i>Secondary</i> zones contain nodes of the
new <i>secondary node</i> type, which can only serve as replicas and do
not vote in master elections.  Secondary zones can be used to make a
copy of the data available at a distant location, or to maintain an
extra copy of the data to increase redundancy or read capacity. [#22483]
</li><br>
<li>
The <code>show plan</code> command now provides an estimated migration completion time. For example:
<pre>
Plan Deploy Topo (12)
State:                 RUNNING
Attempt number:        1
Started:               2014-01-14 17:35:09 UTC
Ended:                 2014-01-14 17:35:27 UTC
Total tasks:           27
 Successful:           12
 Incomplete:           15
Incomplete tasks
   3 partition migrations queued
   1 partition migrations running
   11 partition migrations succeeded, avg migration time = 550164 ms.
<b>Estimated completion:  2014-01-14 19:57:37 UTC</b>
</pre>
[#22183]
</li><br>
<li>
A new read consistency option has been added for this release.
<code>oracle.kv.Consistency.NONE_REQUIRED_NO_MASTER</code> can now be
used to specify that the desired read operations must always be serviced
by a replica, never the master. For read-heavy applications (ex. analytics),
it may be desirable to isolate read requests so that they are performed
only on replicas, never a master; reducing the load on the master. The
preferred mechanism for achieving this sort of read isolation is the new
secondary zone feature; which users are encouraged to employ for this
purpose. But for cases where the use of secondary zones is not desired
or impractical, <code>oracle.kv.Consistency.NONE_REQUIRED_NO_MASTER</code>
can be used to achieve a similar effect, without the additional resources
that secondary zones may require. [#22338]
</li><br>
<li>
The following methods have been added:
<p>
<ul>
<li><code>KVStoreConfig.setReadZones()</code></li>
<li><code>KVStoreConfig.getReadZones()</code></li>
</ul>
<p>
These methods make it possible to require that read operations only be
performed on nodes located in the specified zones.
</li><br>
<li>
The <i>show plans</i> command has been changed so that a range of plan
history can be specified.  With no arguments, <i>show plans</i> now
displays only the ten most recently created plans, but new arguments
can be used to select ranges by creation time and by plan id.  Issue
the command "show plans -help" to see the complete set of options.
</li><br>
<li>
The makebootconfig utility has a new optional <code>-runadmin</code>
command-line argument, which allows the SNA to force the start of a bootstrap
admin even if the value of <code>-admin</code> is set to 0.
<p>
The option <code>-port</code> of <i>plan deploy-admin</i> within the admin CLI
has been changed to be able to control the start of the admin web service. No
web service of the admin will be started if the <code>-port</code> is set to
0 in deploying.
<p>
Users can also change the http port of an admin after deployment via
<i>plan change-param</i> command of admin CLI to change the setting for whether
an admin runs a web server. [#22344]
</li><br>
<li>
The <i>plan change-param</i> command has been changed to allow changing the
parameters for a single admin service. [#22244]
</li><br>
<li>
NoSQL topology information is stored both in the Admin services and on
Storage Nodes, and can become inconsistent if topology changing plans
such as deploy-topology and migrate-sn are canceled before
completion. Inconsistencies can be repaired by redeploying the target
topology. In this release, a "plan repair-topology" command is also provided
as an additional way of repairing topology inconsistencies. The verify
configuration command now generates recommendations for when it may be
beneficial to use repair-topology. [#22753]
</li><br>
</ol>
<h3><a name="BugFixes">Bug and Performance Fixes</a></h3>
<ol>
<li>
The makebootconfig command now prints a message when it declines to
overwrite existing configuration files. [#23012]
</li><br>
<li>
The "plan remove-admin" now permits removal of an Admin that is hosted
by Storage Node that is not running. [#23061]
</li><br>
<li>
Fixed a bug that sometimes caused a duplication of the admin section
in a Storage Node's config.xml file. As a result, the "plan
change-parameters" command, when applied to an Admin service with this
configuration irregularity, could unexpectedly have no effect.  The
bug could be provoked by attempting to deploy an Admin that is already
deployed; but it could also happen when re-executing a failed 
"plan migrate-storagenode" command. [#23152]
</li><br>
<li>
Fixed a problem that caused the <code>topology redistribute</code>
command to ignore storage directory settings when creating new
replication nodes. [#23161]
</li><br>
<li>
Previously, when there was no activity during a RepNode's
metrics-gathering period (the <i>statsInterval</i>), the previous
period's metric values would be reported via JMX and SNMP.  This
behavior has changed so that the metrics are updated at every
interval. [#22842][#22537]
</li><br>
<li>
NoSQL DB automatically adjusts mastership identity so that master
nodes are distributed across a store for optimal performance. 
Fixed a problem that prevented Master Balancing from being performed
across multiple zones. [#22857]
</li><br>
<li>
Modified the LOB implementation to repeat calls to
<code>InputStream.skip</code> as needed to position the input stream to
the start location, so long as the calls return non-zero values.
An <code>IllegalArgumentException</code> will be thrown if the calls do
not advance the stream to the required start location.
</li><br>
</ol>
<!-- ========================================================= -->
<h3><a name="Utilities">Utility Changes:</a></h3>
<!-- ========================================================= -->
<ol>
<li>
The administrative and data command line interfaces (CLI) have been
merged into a single program.  The usage of the merged CLI is
compatible with most old usage but has additional options that allow
it to work for administrative operations, data operations, or both.
This change requires the use of kvstore.jar for data operations where
in previous releases, the data CLI only required kvcli.jar, which
depended on kvclient.jar.
</li><br>
<li>
The CLI has been enhanced with commands necessary to manage tables,
indexes, security information, and zones.
</li>
</ol>
<!-- ========================================================= -->
<h3><a name="Documentation">Documentation Changes:</a></h3>
<!-- ========================================================= -->
<ol>
<li>
  With the introduction of the tabular data model and secondary
  indexes, a new <a
  href="http://docs.oracle.com/cd/NOSQL/html/GettingStartedGuideTables/index.html">
  Getting Started with the Table API</a> guide has been added.
</li><br>
<li>
  With the introduction of the new security features, a new <a
  href="http://docs.oracle.com/cd/NOSQL/html/SecurityGuide/index.html">Security
  Guide</a> has been added.
</li>
</ol>
<!-- ========================================================= -->
<h3><a name="Packaging">Packaging Changes:</a></h3>
<!-- ========================================================= -->
<ol>
<li>
  The versions of the Avro and Jackson libraries bundled with Oracle
  NoSQL database have been upgraded to the more recent Avro 1.7.6 and
  Jackson 1.9.3.  These versions are compatible with the previous API
  versions.
</li>
</ol>
<!-- ============================================================ -->
<hr>
<h3 align="center"><u>Changes in 12cR1.2.1.57</u></h3>
<h3><a name="NewFeatures2157">New Features</a></h3>
<ol>
<li>
The new method <code>KVStore.appendLOB()</code> now permits appending to an
existing LOB (Large Object). As part of this change, the
method <code>PartialLOBException.isPartiallyDeleted()</code> has been deprecated
in favor of the new
method: <code>PartialLOBException.getPartialState()</code>. Please consult the
javadoc associated with these new methods, as well as the updated doc for the
interface <code>KVLargeObject</code>, for a detailed description of this new
functionality.
<p>
This release is backwards compatible with LOBs created in previous releases,
with one exception: Only LOBs created in this, or a later, release support the
append operation. Attempts to use the append operation on LOBs created in
previous releases will result in the method throwing an
UnsupportedOperationException.
<p>
LOBs created in this release cannot be read or deleted by clients using earlier
releases. Such operations will typically fail with a ConcurrentModificationException.
Please ensure that all clients are updated to this release before creating new
LOBs.
[#22876]
</li><br>
<li>
GC log files for the Admin and RepNode services are now generated by default and
placed in the <code>KVROOT/&lt;storename&gt;/log</code> directory (the standard
location for all NoSQL related logging information). This default behavior only
applies when using JDK release 1.7 or a later release, since gc log rotation is
only supported in the more recent JDKs. The logging has minimal resource
overheads. Having these log files readily available, conforms to deployment best
practices for production java applications making it simpler to diagnose GC
issues should the need arise. [#22858]
</li><br>
</ol>
<h3><a name="BugFixes2157">Bug and Performance Fixes</a></h3>
<ol>
<li>
The heap requirement of the Admin service, when operating on a store that has
undergone numerous changes, has been reduced. [#21143]
</li><br>
<li>
Fixed a bug in the Admin CLI "show plan -id &lt;id&gt;" command, which
resulted in the omission of information about partition migration
tasks from the plan history report. The command now correctly includes
information about partition migrations. [#22611]
</li><br>
<li>
Reduce internal timeout values, associated with the network connection
between a master and a replica, to permit faster master failover upon
encountering a network hardware failure. [#22861]
</li><br>
<li>
An attempt to resume a failed put operation on a LOB larger than 3968K bytes
could result in an incorrect ConcurrentModificationException in some
circumstances. The bug has been fixed in this release. [#22876]
</li><br>
<li>
Changed the way plans are represented in the Admin's memory.
Previously, there was no limit on the potential size of the in-memory
representation of currently active and historical plans.  With this
fix, only active plans are kept in memory. [#22963]
</li><br>
<li>
Eliminated deadlocks in plan management in the Admin. [#22992]
</li><br>
<li>
A bug in the argument checking for the
<code>StoreIteratorConfig</code> setter methods has been
fixed. [#23010]
</li><br>
<li>
The makebootconfig command now prints a message when it declines to
overwrite existing configuration files. [#23012]
</li><br>
<li>
The Replication Node configuration has been tuned to reduce CPU
utilization when the Replication Node's cache is smaller than
required, and cache eviction is taking place.  [#23026]
</li><br>
<li>
The remove-admin command now permits removal of an Admin that is
hosted by Storage Node that is not running. [#23061]
</li><br>
<li>
The show plans command could sometimes cause a crash in the Admin CLI
because it would consume too much memory. This has been fixed. [#23105].
</li><br>
</ol>
<!-- ============================================================ -->
<hr>
<h3 align="center"><u>Changes in 12cR1.2.1.54</u></h3>
<h3><a name="NewFeatures2154">New Features</a></h3>
<ol>
<li>
Oracle NoSQL Database now offers a client only package. Oracle NoSQL
Database Client Software Library is licensed pursuant to the Apache
2.0 License (Apache 2.0).  The Apache License and third party notices
for the NoSQL DB Client Software Library may be viewed at this
<a href="http://www.oracle.com/technetwork/products/nosqldb/documentation/nosql-db-agpl-license-1432845.txt"> location </a>
or in the downloaded software.
</li><br>
<li>
A new overloading of the <code>KVStore.storeIterator()</code>
and <code>KVStore.storeKeysIterator()</code> implements <i>Parallel
Scans</i>. The other <code>storeIterator()</code> methods scan all
shards and Replication Nodes in serial order. The new Parallel
Scan methods allow the programmer to specify a number of client-side
threads that are used to scan Replication Nodes in parallel. [#22146]
</li><br>
</ol>
<h3><a name="BugFixes2154">Bug and Performance Fixes</a></h3>
<ol>
<li>
Improved error messages in the Data Command Line Interface
(kvshell). For example, a put command with invalid inputs might have
returned this error message in the past:
<pre>
kvshell-> put -key /test -value ./emp.insert -file -json Employee
  Could not create JSON from input:
  Unable to serialize JsonNode
</pre>
but will now produce this more useful response:
<pre>
kvshell-> put -key /test -value ./emp.insert -file -json Employee
Exception handling command put -key /test -value ./emp.insert -file -json Employee:
  Could not create JSON from input:
  Expected Avro type STRING but got JSON value: null in field Address of Employee
</pre>
[#22791]
</li><br>
<li>
Fixed a bug when using the plan deploy-admin command. In some cases,
if an Admin service encountered an error at start up, the process would
become unresponsive. The correct behavior is for the process to shut
down and be restarted by its owning Storage Node. [#22908]
</li><br>
</ol>
<!-- ============================================================ -->
<hr>
<h3 align="center"><u>Changes in 12cR1.2.1.24</u></h3>

<h3><a name="BugFixes2124">Bug and Performance Fixes</a></h3>
<ol>
<li>
Under certain circumstances, a replication node which was on the verge
of shutting down or in the midst of transitioning from master to
replica state could experience this failure while cleaning up
outstanding requests. Since the node would automatically restart,
and the operation would be retried, the failure was transparent to the
application, but could cause an unnecessary node failover. This has
been fixed.
<pre>
java.lang.IllegalStateException: Transaction 30 detected open cursors while aborting
    at com.sleepycat.je.txn.Txn.abortInternal(Txn.java:1190)
    at com.sleepycat.je.txn.Txn.abort(Txn.java:1100)
    at com.sleepycat.je.txn.Txn.abort(Txn.java:1073)
    at com.sleepycat.je.Transaction.abort(Transaction.java:207)
    at oracle.kv.impl.util.TxnUtil.abort(TxnUtil.java:80)
    at oracle.kv.impl.api.RequestHandlerImpl.executeInternal(RequestHandlerImpl.java:469)
    at oracle.kv.impl.api.RequestHandlerImpl.access$300(RequestHandlerImpl.java:122)
    at oracle.kv.impl.api.RequestHandlerImpl$2.execute(RequestHandlerImpl.java:301)
    at oracle.kv.impl.api.RequestHandlerImpl$2.execute(RequestHandlerImpl.java:290)
    at
oracle.kv.impl.fault.ProcessFaultHandler.execute(ProcessFaultHandler.java:135>
</pre>
[#22152]
</li><br>
<li>
In past releases of NoSQL DB, a replication node which transitioned
from master to replica state would have to close and reopen its
database environment as part of the change in status. This transition
has now been streamlined so that in the majority of cases, the
database environment is not perturbed, the transition requires fewer
resources, and the node is more available.
[#22627]
</li><br>
<li>
The <code>plan deploy-topology</code> command has additional
safeguards to increase the reliability of the topology rebalance and
redistribute plans. When moving a replication node from one Storage
Node to another, the command will now check that the Storage Nodes
involved in the operation are up and running before any action is
taken. [#22850]
</li><br>
<li>
Under certain circumstances it was possible for a replication node to
use out of date master identity information when joining a
shard. This could cause a delay if the targeted node was
unavailable. This has been fixed. [#22851]
</li><br>
<li>
Under certain circumstances operations would end prematurely
with oracle.kv.impl.fault.TTLFaultException. This exception is now
handled internally by the server and client library and the operation
is retried. If the fault condition continues the operation will
eventually fail with a oracle.kv.RequestTimeoutException. [#22860]
</li><br>
<li>
Previously, there were cases where a replication node would require
the transfer of a copy of the shard data in order to come up and join
the shard, even though it was unnecessary. This has been
fixed. [#22782]
</li><br>
<li>
When new storage nodes are added to an Oracle NoSQL DB deployment and
a new topology is deployed, the store takes that opportunity to
redistribute master roles for optimal performance. In some cases, the
store might not notice the new storage nodes until other events, such
as failovers or mastership changes had occurred, which caused a delay
in master balancing. This has been fixed. [#22888]
</li><br>
<li>
The setting of the JE configuration parameter:
je.evictor.criticalPercentage used by the store has been corrected. It
used to be set to 105 and has been changed to 20. This new setting
will provide better cache management behavior in cases where the data
set size exceeds the optimal memory settings. [#22899]
</li><br>
</ol>
<h3><a name="UtilityAndDocChanges2124">Utility and Documentation Changes</a></h3>
<ol>
<li>
A timestamp has been added to the output of the CLI "ping" command. [#22859]
</li><br>
</ol>

<!-- ============================================================ -->
<hr>
<h3 align="center"><u>Changes in 12cR1.2.1.19</u></h3>

<h3><a name="documentation2119">Documentation Changes</a></h3>
<ol>
<li>
This release includes a new document, Oracle NoSQL Database
Availability and Failover. It explains the general concepts and
issues surrounding data availability when using Oracle NoSQL Database.
The intended audiences for this document are system architects and
developers. The new information can be found under the "For the
Developer" section in the documentation index page.
</li><br>
<li>
Clarify the instructions for adding .avsc files to the classpath for
the example on Avro bindings in &lt;KVHOME&gt;/examples/avro.
Improve the error message when the .avsc files are not properly available.
</li><br>
</ol>

<h3><a name="BugFixes2119">Bug and Performance Fixes</a></h3>
<ol>
<li>
Increased an internal parameter for lock timeouts from 500ms to 10
seconds. Since NoSQL DB ensures that data access is deadlock free, the
small timeout values were unnecessary and could cause spurious errors
in the face of transient network failures. [#22583]
</li><br>
<li>
Changing the store topology through the <code>plan
deploy-topology</code> command could result in the following
error if there was a transient network failure, or if the movement of
the replication node took longer than a few seconds. Although the store
state was still consistent, and the command could be manually retried,
the command should be more resilient to communication glitches.
<pre>
... [admin1] Task 2/RelocateRN ended in state ERROR with
java.lang.RuntimeException: Time out while waiting for rg4-rn1 to come
up on sn1 and become consistent with the master of the shard before
deleting the RepNode from its old home on sn4 2/RelocateRN failed.

java.lang.RuntimeException: Time out while waiting for rg4-rn1 to come
up on sn1 and become consistent with the master of the shard before
deleting the RepNode from its old home
</pre>
<p>
The command will now adjust waiting times and retry appropriately to
ascertain whether the movement of a
replication node has finished.[#22596]
</li><br>
<li>
Fixed a bug where a replication node would not restart automatically if
the directory containing its data files was removed, or its data files
were corrupted, but were later repaired. [#22626]
</li><br>
<li>
Added additional testing to reinforce the existing, correct behavior
that a client directs write requests to the authoritative master in a
segmented network split brain scenario. [#22636]
</li><br>
<li>
In some cases, the <code>java -jar kvstore.jar ping</code> command
could generate spurious messages about components that are no longer
legitimately within the store.
<pre>
Failed to connect to service commandService

Connection refused to host: 10.32.17.12; nested exception is:
  java.net.ConnectException: Connection refused
        SNA at hostname:localhost registry port: 6000 has no available
        Admins or RNs registered.
</pre>
In particular, these messages could happen for bootstrap Admins on
Storage Nodes that do not host deployed Admin Services. While the
store was consistent, the error messages were confusing and have been
removed. [#22639]
</li><br>

<li>
Fixed a small timing window in Replication Node master transfer that
could incorrectly cause the transfer transaction catch up point to
regress, when a master transfer is occurring under heavy application
load. The result is that shard mastership can take too long a time or
too short a time to transfer. If the transfer time is too short, the
target master may not be optimally caught up, and a third member of
the shard may detect this and throw an exception.
[#22658]
</li><br>
<li>
Preemptively shut down and restart the replication node when a node
transitions from master to replica, to reduce GC cost from refreshing
the database environment.
[#22658]
</li><br>
<li>
Made changes to the NoSQL client library to adapt to replication node
failures more rapidly, by retrying or forwarding data requests when it
detects that its original target is unavailable sooner.
[#22661]
</li><br>
<li>
A NoSQL deployment could see this transient error when
undergoing topology changes. Although the store remained consistent, the
error messages were confusing and could incorrectly cause a <code>plan
deploy-topology</code> command to fail. This has been corrected.
<pre>
 ... INFO [rg1-rn1] Failed pushing entire topology push to rg1-rn3
updating from topo seq#: 0 to 1001 Problem:Update to topology seq#
1001 failed ... oracle.kv.impl.fault.OperationFaultException:
  Update to topology seq# 1001 failed
    at oracle.kv.impl.rep.admin.RepNodeAdminImpl$6.execute(RepNodeAdminImpl.java:261)
    at oracle.kv.impl.fault.ProcessFaultHandler.execute(ProcessFaultHandler.java:169)
    at oracle.kv.impl.rep.admin.RepNodeAdminFaultHandler.execute(RepNodeAdminFaultHandler.java:117)

 ...INFO [rg1-rn3] Topology update skipped. Current seq #: 1001 Update seq #: 1001
</pre>
[#22678]
</li><br>
<li>
Fixed a bug where a replication node which experiences an out of
memory error did not restart automatically.
[#22679]
</li><br>
<li>
Corrected the default calculation of available Storage Node memory when
the Storage Node has been configured without a value for the bootstrap
memory_mb parameter. In the past, the calculation was done using units
of decimal megabytes, rather than MB, resulting in an overestimation
of the appropriate replication node heap size. This default
calculation is only used if the store has been configured without any
bootstrap value for the memory_mb property, and the memory_mb storage node
parameter has never been set.
[#22687]
</li><br>
<li>
Update the Storage Node more quickly about the replica/master status
of the replication nodes it hosts. The fix applies when executing the
<code>plan deploy-topology</code> command on a store that contains Storage Nodes
that have capacity values greater than 1, and can host multiple
Replication Nodes. A delay in notifying the Storage Node of its
replication nodes status can make the distribution of mastership
responsibilities less optimal.
[#22689]
</li><br>
<li>
Fixed a bug where the Admin service became unresponsive when executing
a <code>plan deploy-topology</code> command. During this time, the
admin service process appeared idle, only burning a second or two of
CPU time once in a while and would not respond to new attempts to
connect with the Admin CLI. The problem would likely only occur in
large clusters with hundreds of components.  [#22694]
</li><br>
<li>
Topology changes invoked by the <code>plan deploy-topology</code>
which result in the movement of a replication node from one storage
node to another are now more resilient to transient network failures.
There are now more advance checks to ensure that the shard and storage
nodes involved in the movement are available and ready to accept the
change. In the advent of a network failure mid-move, the command is
better at handling retries issued by the system administrator.
[#22722]
</li><br>
<li>
Fixed a bug where application requests failed to be processed while the
store is executing topology changes that require partition migration
under heavy load.
[#22778]
</li><br>
<li>
Adjust the default replication node garbage collection parameters to
be more optimal, reducing CPU utilization in some cases.
[#22779]
</li><br>
<li>
Reduce the time taken for a replica Replication Node to become up to
date and available to handle application requests when it has fallen
significantly behind due to downtime or to network communication
failures. Previously, it exited and restarted the process before
starting the catch up stage, but will now skip the restart.
[#22783]
</li><br>
<li>
Fixed a bug where an internal queue in the Storage Node could fill up
if its Replication Node repeatedly and unsuccessfully attempts to
restart, as might happen when a resource is unavailable. In that case,
the Storage Node was no longer able to automatically restart
the replication node, and would have to be rebooted.
[#22786]
</li><br>
<li>
Fixed a bug where a Replication Node that has been stopped due to
repeated errors, perhaps due to a lack of resource, and then
re-enabled with the "plan start-service" command, still did not
restart. [#22828]
</li><br>
<li>
Fixed a bug where the following null pointer exception could happen
for a restarting Replication Node. The problem was transient.
<pre>
   INFO [sn1] rg2-rn2: ProcessMonitor: startProcess
   INFO [sn1] rg2-rn2: ProcessMonitor: stopProcess
   SEVERE [sn1] rg2-rn2: ProcessMonitor: Unexpected exception in
MonitorThread:
        java.lang.NullPointerExceptionjava.lang.NullPointerException
	at oracle.kv.impl.sna.ProcessMonitor$MonitorThread.run(ProcessMonitor.java:404)
</pre>
[#22830]
</li><br>
<li>
Improve the client library's interpretation of UnknownHostException
and ConnectIOException so that it more rapidly detects a network
problem and updates its set of unavailable replication nodes.
[#22841]
</li><br>

<!--
Fixed a bug in the unadvertised command "topology move-repnode". In
the past, the move-repnode command did not use available storage
directories on the target SN. This has been fixed, so a RN that is
moved will use any available storage dirs on the target SN. The fix
is not documented because this in an undocumented command. [#22648]
-->
</ol>
<!-- ============================================================ -->
<hr>
<h3 align="center"><u>Changes in 12cR1.2.1.8</u></h3>
<h3><a name="Features218">New Features</a></h3>
<ol>
<li>
This release includes support for upgrading the NoSQL DB software (client or
server) without taking the store offline or without significant impact to
ongoing operations. In addition, upgrades can be made incrementally, that is,
it should not be necessary to update the software on every component at the
same time. This support includes client and server code changes and new
command line interface (CLI) commands. [#22421]
<p>
The new CLI commands provide the
administrator tools to help with the upgrade process. Using these commands, the
general upgrade procedure is:
<ol>
    <li>Install the new software on a Storage Node running an admin
        service<sup>1</sup>.
    <li>Install the new client and connect to the store.
    <li>Use the <code>verify prerequisite</code> command to verify that the
        entire store is at the proper software version to be upgraded (All
        2.0 versions of NoSQL DB will qualify as prerequisites).
    <li>Use <code>show upgrade-order</code> to get an ordered list of nodes to
        upgrade.
    <li>Install the new software on the Storage Nodes (individually or in groups
        based on the ordered list).
    <li>Use the <code>verify upgrade</code> to monitor progress and verify that
        the upgrade was successful.
</ol><br>
<sup>1</sup> In future releases this step will not be necessary
<p>
If the upgrade procedure is interrupted steps 4-6 can be repeated as necessary
to complete the upgrade.
</li>
</ol>

<h3><a name="BugFixes218">Bug and Performance Fixes:</a></h3>
<ol>
<li>
Unless configured specifically by the application, NoSQL DB specifies the
-XX:ParallelGCThread jvm flag for each Replication Node process to
indicate the number of garbage collector threads that the process
should use. In the past, the algorithm in use generated a minimum
value of 1 thread. After more testing, the minimum value has been
raised to the min(4, &lt;the number of cores on the node&gt;). [#22475]
</li><br>
</ol>

<h3><a name="APIChanges218">API Changes</a></h3>
<ol>
<li>

The admin command line interface (CLI) provides the following new
commands [#22422]:
<p>
<code>verify prerequisite [-silent] [-sn snX]*</code>
<p>
This command will verify that a set of storage nodes in the store meets the
required prerequisites for upgrading to the current version and display the
components which do not meet prerequisites or cannot be contacted. It will also
check and report an illegal downgrade situation where the installed software is
of a newer minor release than the current version. In this command the current
version is the version of the software running the command line interface. If no
storage nodes are specified, all of the nodes in the store will be checked.
<p>
<code>verify upgrade [-silent] [-sn snX]*</code>
<p>
This command will verify that a set of storage nodes in the store has been
successfully upgraded to the current version and display the components which
have not yet been upgraded or cannot be contacted. In this command the current
version is the version of the software running the command line interface. If no
storage nodes are specified, all of the nodes in the store will be checked.
<p>
<code>show upgrade-order</code>
<p>
This command will display the list of storage nodes in the order that they
should be upgraded to maintain the store's availability. This command will
display one or more storage nodes on a line. Multiple storage nodes on a line
are separated by a space. If multiple storage nodes appear on a single line,
then those nodes can be safely upgraded at the same time. When multiple nodes
are upgraded at the same time, the upgrade must be completed on all nodes
before the nodes next on the list can be upgraded.
<p>
The <code>verify [-silent]</code> command has been deprecated and is replaced by
<code>verify configuration [-silent]</code>. The <code>verify [-silent]</code>
command will continue to work in this release.
</li>
</ol>

<h3><a name="UtilityAndDocChanges218">Utility and Documentation Changes</a></h3>
<ol>
<li>In this release, the sample code provided by the utility
  class <a href="javadoc/examples/schema/WriteOperations.html/"> <code>WriteOperations</code></a>
  (located in the <code>examples</code> directory) now includes methods that
  perform write operations for <em>large objects</em> (or <em>LOB</em>,
  see <a href="javadoc/oracle/kv/lob/KVLargeObject.html/"> <code>KVLargeObject</code></a>). The
  new utility methods added in this release will properly retry the associated
  LOB operation when
  a  <a href="javadoc/oracle/kv/FaultException.html/"> <code>FaultException</code></a>
  is encountered. Prior to this release,
  the <a href="javadoc/examples/schema/WriteOperations.html/"> <code>WriteOperations</code></a>
  utility only provided retry methods for objects that are not large
  objects. [#21966]
</li><br>
<li>
The number of JE lock tables used by Replication Nodes (controlled via
the <i>je.lock.nLockTables</i> JE configuration parameter) has been increased from
1 to 97. This change helps improve performance of applications characterized by
very high levels of concurrent updates, by reducing lock contention. [#22373]
</li><br>
<li>
The Administration CLI now permits the creation of multiple Datacenters.
By choosing Datacenter replication factors so that each Datacenter holds
less than a quorum of replicas, this change makes it possible to create
store layouts where the failure of a single Datacenter does not result
in the loss of write availability for any shards in the store.  In the
current release, nodes in any Datacenter can participate in master
elections and contribute to durability acknowledgments.  As a
consequence, master failover and durability acknowledgments will take
longer if they involve datacenters that are separated by large
distances.  Future releases will provide greater flexibility in this
area.  [#20905]
</li>
</ol>

<!-- ========================================================= -->
<hr>
<h3 align="center"><u>Changes in 11gR2.2.0.39</u></h3>
<h3><a name="Features2039">New Features</a></h3>
<ol>
<li>
An integration with Oracle Coherence has been provided that allows
Oracle NoSQL Database to be used as a cache for Oracle Coherence
applications, also allowing applications to directly access cached
data from Oracle NoSQL Database.  This integration is a feature of the
Enterprise Edition of the product and implemented as a new, independent jar
file.  It requires installation of the Oracle Coherence product as
well.  The feature is described in the
<a href="GettingStartedGuide/coherence.html">Getting Started
Guide</a> as well as the <a
href="javadoc/oracle/kv/coherence/package-summary.html">javadoc</a>. [#22291].
</li><br>
<li>
The Enterprise Edition now has support for semantic technologies.
Specifically, the Resource Description Framework (RDF), SPARQL query
language, and a subset of the Web Ontology Language (OWL) are now
supported.  These capabilities are referred to as the RDF Graph
feature of Oracle NoSQL Database.  The RDF Graph feature provides a
Java-based interface to store and query semantic data in Oracle NoSQL
Database Enterprise Edition. The feature is described in the
<a href="RDFGraph/index.html">RDF Graph</a> manual.
</li><br>
</ol>
<h3><a name="BugsPatch2039">Bug Fixes</a></h3>
<ol>
<li>
The preferred approach for setting NoSQL DB memory resources is to
specify the memory_mb parameter for each SN when running the
makebootconfig utility, and to let the system calculate the ideal
Replication Node heap and cache sizes. However, it is possible to override the
standard memory configurations by explicitly setting heap and cache
sizes using the Replication Node javaMiscParams and cacheSize
parameters. In past releases, setting the explicit values worked
correctly when using the plan change-parameters command, but did not
work correctly when using the change-policy command. This has been
fixed, so that if desired, one can use the change-policy command for
the javaMiscParams and cacheSize parameters to override the default
memory allocation heuristics. [#22097]
</li><br>
<li>
A NoSQL DB deployment that executes on a node with no network
available, as might happen when running a NoSQL DB demo or tutorial,
would fail with this error:
<pre>
java.net.InetAddress.getLocalHost() returned loopback address:&lt;hostname&gt; and
  no suitable address associated with network interfaces.
</pre>
This has been fixed. [#22252]
</li><br>
</ol>
<h3><a name="APIPatch2039">API Changes</a></h3>
<ol>
<li>
Prior to this release, if a write operation encountered an exception from the underlying persistent store indicating that the write completed on the shard's master but not necessarily on the desired combination of replicas within the specified time interval, then that exception would be swallowed and thus never propagated to the client. Originally, this behavior was considered desirable because not only is that exception rare (because of various preceding checks performed by the implementation), but swallowing the exception would keep the API simple by avoiding the introduction of an additional exception and/or additional communication at the API level. After further thought and discussion, the team concluded that clients should know when a write operation fails to complete because of such an exception. As a result, when such a condition occurs during a write operation, a <a href="javadoc/oracle/kv/RequestTimeoutException.html/"> <code>RequestTimeoutException</code></a> will now be propagated to the client; wrapping the original exception from the underlying persistent store as the cause. For additional information, including strategies one might employ when this exception is encountered, refer to the <a href="javadoc/oracle/kv/RequestTimeoutException.html/"> <code>RequestTimeoutException</code> <em>javadoc</em></a>.
<p>
This has been fixed. [#21210]
</li><br>
<li>
A new parameter has been added which controls the display of records
in exception and error messages. When <code>hideUserData</code> is set
to true, as it is by default, error messages which are printed to the
server side logs or are displayed via the show CLI commands replace
any key/values with the string "[hidden]". To see the actual record content
in errors, set the parameter to false. [#22376]
</li><br>
</ol>
<h3><a name="UtilityPatch2039">Utility and Documentation Changes</a></h3>
<ol>
<li>
In previous releases, information about errors that occurred during
NoSQL DB component start up as a result of a <code<>plan
deploy-topology</code> command would often be visible only within
the NoSQL DB logs, which made
installation troubleshooting difficult. In this
release, such start up errors can now be seen via the Admin CLI
<code>show plan -id &lt;id&gt;</code> command. [#22101]
</li><br>
<li>
The Storage Node Agent exposes MBeans on a non-default MBeanServer
instance.  In this release, the non-default MBeanServer now exposes
the standard JVM platform MBeans as well as those relating only to
Oracle NoSQL Database.
</li><br>
<li>
In both SNMP and JMX interfaces, the new totalRequests metric is now available.  This metric counts the number of multi-operation sequences that occurred during the sampling period.
</li><br>
<li>
Prior to this release, the product was compiled and built against the 1.x version of Hadoop (CDH3). Thus, when employing a previous release, if one were to run the <code>examples.hadoop.CountMinorKeys</code> example against a cluster based on the 2.x version of Hadoop (CDH4), the MapReduce job initiated by that example would fail as a result of an <code>IncompatibleClassChangeError</code>; which is caused by an incompatibility introduced in <code>org.apache.hadoop.mapreduce.JobContext</code> between Hadoop 1.x and Hadoop 2.x. This failure occurs whether the example is compiled and built against Hadoop 1.x or Hadoop 2.x. Because the product's customer base almost exclusively uses Hadoop 2.x, this release will provide support for Hadoop 2.x instead of 1.x. Future releases may revisit support for both Hadoop version paths, but doing so will involve refactoring the codebase and its associated release artifacts, as well as substantial changes to the product's current build process.
<p>
Support of Hadoop 2.x (CDH4) has been provided. [#22157]
</li><br>
<li>
The java -jar kvstore.jar makebootconfig -mount flag has been changed
to -storagedir. The "plan change-mountpoints -path &lt;storage
directory&gt;" command is deprecated in favor of "plan
change-storagedir -storagedir &lt;storage directory&gt;". [#21880]
</li><br>
<li>
The concept of Storage Node capacity is better explained in the
Administrator's Guide.
</li><br>
<li>
The Administrator's Guide has a revamped section on how to calculate the
resources needed for operating a NoSQL DB deployment.
</li><br>
</ol>
<!-- ========================================================= -->
<hr>
<h3 align="center"><u>Changes in 11gR2.2.0.26</u></h3>
<h3><a name="Features2026">New Features:</a></h3>
<ol>

<li>
This release adds the capability to remove an Admin service replica.
If you have deployed more than one Admin, you can remove one of them
using the following command:
<blockquote>
<pre>
plan remove-admin -admin &lt;adminId&gt;
</pre>
</blockquote>
<p>
You cannot remove the sole Admin if only one Admin instance is
configured.
</p><p>
For availability and durability reasons, it is highly recommended that
you maintain at least three Admin instances at all times.  For that
reason, if you try to remove an Admin when the removal would result in
there being fewer than three, the command will fail unless you give
the <i>-force</i> flag.
</p><p>
If you try to remove the Admin that is currently the master,
mastership will transfer to another Admin.  The plan will be
interrupted, and subsequently can be re-executed on the new master
Admin.  To re-execute the interrupted plan, you would use this command:
</p>
<blockquote>
<pre>
plan execute -id &lt;planId&gt;
</pre>
</blockquote>
</li><br>

<li>
The Admin CLI verify has an added check to verify that the Replication
Nodes hosted on a single Storage Node have memory settings that fit
within the Storage Node's memory budget. This guards against mistakes
that may occur if the system administrator overrides defaults and
manually sets Replication Node heap sizes.[#21727]
</li><br>

<li>
The Admin CLI verify command now labels any verification issues as
violations or notes. Violations are of greater importance, and the
system administrator should determine how to adjust the system to
address the problem. Notes are warnings, and are of lesser
importance. [#21950]
</li><br>

</ol>

<h3><a name="BugFixes2026">Bug fixes:</a></h3>
<!-- ========================================================= -->
<ol>

<li>
Several corrections were made to latency statistics. These corrections apply
to the service-side statistics in the Admin console, CLI <code>show perf</code>
command, .perf files and .csv files, as well as the client-side statistics
returned by KVStore.getStats. However, corrections to the 95% and 99% values do
not apply to the client-side statistics, since these values do not appear in
the client-side API.
  <ul>
      <li>The definition of latency has been corrected for the "multi"
      operation requests (multiGet, multiDelete, execute, etc).  These are
      labeled "multi" in the <code>Op Type</code> column where latency
      information is displayed.  The previous definition was "latency in
      milliseconds per <em>operation</em>" while the new definition is "latency
      in milliseconds per <em>request</em>".  In other words, for a "multi"
      operation request, latency now applies to the entire request rather than
      to each operation.  For "single" operation requests, the definition of
      latency has not changed.
      </li>
      <li>To go along with the change above, a new column containing the number
      of requests in the sample has been added to all latency information
      displays: <code>TotalReq</code>.  This is also available for client-side
      statistics using the new <code>OperationMetrics.getTotalRequests</code>
      method.  For "multi" operation requests, the total number of requests is
      normally smaller than the total number of operations (the
      <code>TotalOps</code> column).  For "single" operation requests, the
      total number of requests and operations are equal.
      </li>
      <li>Improved the consistency of the values reported in each sample so
      that, for example, the minimum latency is always less than the maximum
      latency.  However, note that statistics are collected without
      synchronization to avoid impacting performance, and for small sample
      sizes the values in a sample are not always accurate or self-consistent.
      </li>
      <li>Fixed a bug that caused the 95% and 99% values to show the maximum
      latency recorded (within 1000 ms), rather than the lowest 95% or 99% as
      intended.  This bug only applied to the "multi" operation requests.
      </li>
      <li>Fixed a bug that caused the 95% and 99% values to sometimes
      mistakenly appear as -1.  These values should only appear as -1 when
      there were no operations in the sample with a latency below 1000 ms.
      </li>
  </ul>
[#21763]
</li><br>

<li>
Modified the Administration Process to allocate ports from within a port range
if one is specified by the <i>-servicerange</i> argument to
the <i>makebootconfig</i> utility. If the argument is not specified the
Administration Process will use any available port. Please see
the <a href="http://docs.oracle.com/cd/NOSQL/html/AdminGuide/"> Admin Guide</a>
for details regarding the configuration of ports used by Oracle NoSQL
Database.  [#21962]
</li><br>

<li>
Modified the replication node to handle the unlikely case that the locally
stored topology is missing. A missing topology results in a
java.lang.NullPointerException being thrown in the TopologyManager and will
prevent the replication node from starting. [#22015]
</li><br>

<li>
Replication Node memory calculations are more robust for Storage Nodes
that host multiple Replication Nodes. In previous releases, using the
plan change-params command to reduce the capacity parameter for a
Storage Node which hosts multiple Replication Nodes could result in an
over aggressive increase in RN heap, which would make the Replication
Nodes fail at start up. The problem would be fixed when a topology was
rebalanced, but until that time, the Replication Nodes were
unavailable. The default memory sizing calculation now factors in the
number of RNs resident on a Storage Node, and adjusts RN heap sizes as
Replication Nodes are relocated by the deploy-topology
command. [#21942]
</li><br>

<li>
Fixed a bug that could cause a NullPointerException, such as the one below,
during RN start-up.  The exception would appear in the RN log and the RN would
fail to start.  The conditions under which this problem occurred include
partition migration between shards along with multiple abnormal RN shutdowns.
If this bug is encountered, it can be corrected by upgrading to the current
release, and no data loss will occur.
<pre>
Exception in thread "main" com.sleepycat.je.EnvironmentFailureException: (JE
5.0.XX) ...  last LSN=.../... LOG_INTEGRITY: Log information is incorrect,
problem is likely persistent. Environment is invalid and must be closed.
    at com.sleepycat.je.recovery.RecoveryManager.traceAndThrowException(RecoveryManager.java:2793)
    at com.sleepycat.je.recovery.RecoveryManager.undoLNs(RecoveryManager.java:1097)
    at com.sleepycat.je.recovery.RecoveryManager.buildTree(RecoveryManager.java:587)
    at com.sleepycat.je.recovery.RecoveryManager.recover(RecoveryManager.java:198)
    at com.sleepycat.je.dbi.EnvironmentImpl.finishInit(EnvironmentImpl.java:610)
    at com.sleepycat.je.dbi.DbEnvPool.getEnvironment(DbEnvPool.java:208)
    at com.sleepycat.je.Environment.makeEnvironmentImpl(Environment.java:246)
    at com.sleepycat.je.Environment.&lt;init&gt;(Environment.java:227)
    at com.sleepycat.je.Environment.&lt;init&gt;(Environment.java:170)
    ...
Caused by: java.lang.NullPointerException
    at com.sleepycat.je.log.entry.LNLogEntry.postFetchInit(LNLogEntry.java:406)
    at com.sleepycat.je.txn.TxnChain.&lt;init&gt;(TxnChain.java:133)
    at com.sleepycat.je.txn.TxnChain.&lt;init&gt;(TxnChain.java:84)
    at com.sleepycat.je.recovery.RollbackTracker$RollbackPeriod.getChain(RollbackTracker.java:1004)
    at com.sleepycat.je.recovery.RollbackTracker$Scanner.rollback(RollbackTracker.java:477)
    at com.sleepycat.je.recovery.RecoveryManager.undoLNs(RecoveryManager.java:1026)
    ... 10 more
</pre>
[#22052]
</li><br>

<li>
Fixed a bug that causes excess memory to be used in the storage engine cache on
an RN, which could result in poor performance as a result of cache eviction and
additional I/O.  The problem occurred only when the
<code>KVStore.storeIterator</code> or <code>KVStore.storeKeysIterator</code>
method was used.  [#21973]
</li><br>

</ol>

<!-- ========================================================= -->
<h3><a name="General2026">Performance and other General Changes:</a></h3>
<!-- ========================================================= -->
<ol>

<li>
The replicas in a shard now dynamically configure the JE property
<i>RepParams.REPLAY_MAX_OPEN_DB_HANDLES</i> which controls the size of the cache
used to hold database handles during replication. The cache size is determined
dynamically based upon the number of partitions currently hosted by the
shard. This improved cache sizing can result in better write performance for
shards hosting large numbers of partitions. [#21967]
</li><br>

<li>
The names of the client and server JAR files no longer include release
version numbers.  The files are now called:
<blockquote>
<pre>
lib/kvstore.jar
lib/kvclient.jar
</pre>
</blockquote>
<p>
This change should reduce the amount of work needed to switch to a new
release because the names of JAR files will no longer change between
releases.  Note that the name of the installation directory continues to
include the release version number. [#22034]
</li><br>

<li>
A SEVERE level message is now logged and an admin alert is fired when the
storage engine's average log cleaner (disk reclamation) backlog increases over
time.  An example of the message text is below.
<pre>
121215 13:48:57:480 SEVERE [...] Average cleaner backlog has grown from 0.0 to
6.4. If the cleaner continues to be unable to make progress, the JE cache size
and/or number of cleaner threads are probably too small. If this is not
corrected, eventually all available disk space will be used.
</pre>
For more information on setting the cache size appropriately to avoid such
problems, see "Determining the Per-Node Cache Size" in the Administrator's
Guide.
[#21111]
</li><br>

<li>
The storage engine's log cleaner will now delete files in the latter portion of
the log, even when the application is not performing any write operations.
Previously, files were prohibited from being deleted in the portion of the log
after the last application write.  When a log cleaner backlog was present (for
example, when the cache had been configured too small, relative to the data set
size and write rate), this could cause the cleaner to operate continuously
without being able to delete files or make forward progress.  [#21069]
</li><br>

<li>
NoSQL DB 2.0.23 introduced a performance regression over R1.2.23. The
kvstore client library and Replication Node consumed a greater
percentage of system CPU time. This regression has been fixed. [#22096]
</li><br>
</ol>

<!-- ============================================================ -->
<hr>
<h3 align="center"><u>Changes in 11gR2.2.0.23</u></h3>

<!-- ========================================================= -->
<h3><a name="Features2023">New Features:</a></h3>
<ol>
<li>
This release provides the ability to add storage nodes to the system
after it has been deployed. The system will rebalance and redistribute
the data onto the new nodes without stopping operations. See Chapter
6, of the Admin
Guide, <a href="AdminGuide/managing-topology.html">Determining
your Store's Configuration</a>, for more details.
<li>
A new <code>oracle.kv.lob</code> package provides operations that can
be used to read and write Large Objects (LOBs) such as audio and video
files. As a general rule, any object larger than 1 MB is a good
candidate for representation as a LOB. The LOB API permits access to
large values without having to materialize the value in its entirety
by providing streaming APIs for reading and writing these objects.
<li>
A C API has been added. The implementation uses Java JNI and requires
a Java virtual machine to run on the client. It is available as a
separate download.
<li>
Added a new remove-storagenode plan. This command will
remove a storage node which is not hosting any NoSQL Database components
from the system's topology. Two examples of when this might be useful
are:
<blockquote>
A storage node was incorrectly configured, and cannot be deployed.<br>
A storage node was once part of a NoSQL Database, but all components have
been migrated from it using the migrate-storagenode command, and the
storage node should be decommissioned.
</blockquote>
[#20530]
</li>
<li>
Added the ability to specify additional physical configuration
information about storage nodes including:
<ul>
  <li>Capacity - the number of RepNodes the SN may host</li>
  <li>Number of CPUs</li>
  <li>Amount of memory to use</li>
  <li>Specific directory paths (mount points) to use for RepNodes</li>
</ul>
This information is used by the system to make more intelligent
choices about resource allocation and consumption.  The administration
documentation discusses how these parameters are set and used.
[#20951]
</li>
<li>Added Avro support.  The value of a kv pair can now be stored in Avro
binary format.  An Avro schema is defined for each type of data stored.  The
Avro schema is used to efficiently and compactly serialize the data, to
guarantee that the data conforms to the schema, and to perform automatic
evolution of the data as the schema changes over time.  Bindings are supplied
that allow representing Avro data as a POJO (Plain Old Java Object), a JSON
object, or a generic Map-like data structure. For more information, see
<a href="GettingStartedGuide/avroschemas.html">Chapter 7 - Avro Schemas</a> and
<a href="GettingStartedGuide/avrobindings.html">Chapter 8 - Avro Bindings</a>
in the <i>Getting Started Guide</i>. The <code>oracle.kv.avro</code> package is
described in the Javadoc.  The use of the Avro format is strongly
recommended.  NoSQL DB will leverage Avro in the future to provide additional
features and capabilities.  [#21213]
</li>
<li>Added Avro support for the Hadoop <code>KVInputFormat</code> classes.
A new <code>oracle.kv.hadoop.KVAvroInputFormat</code> class returns Avro
<code>IndexedRecord</code>s to the caller. When this class is  used in
conjunction with Oracle Loader for Hadoop, it is possible to read data
directly from NoSQL Database using OLH without using an interim Map-Reduce
job to store data in HDFS. [#21157]
</li>
<li>Added a feature which allows Oracle Database External Tables to be
used to access Oracle NoSQL Database records. There is more
information in javadoc for the
<a href="javadoc/index.html?oracle/kv/exttab/package-summary.html">
<code>oracle.kv.exttab</code></a> package and an "cookbook" example in the
<a href="examples/index.html?externaltables/package-summary.html">
<code>examples/externaltables</code></a> directory. [#20981]
</li>
</ol>
<!-- ========================================================= -->
<h3><a name="General2023">Performance and other General Changes:</a></h3>
<!-- ========================================================= -->
<ol>
<li>
The following new methods:
<ul>
<li><code>KVStoreConfig.setOpenTimeout()</code></li>
<li><code>KVStoreConfig.getOpenTimeout()</code></li>
<li><code>KVStoreConfig.setReadTimeout()</code></li>
<li><code>KVStoreConfig.getReadTimeout()</code></li>
</ul>
have been added to allow clients to configure the socket timeouts used to make
client requests. Please review the javadoc for details.
<p>
R1 installations must ensure that the software on the storage nodes has
been upgraded as described in the upgrade documentation
accompanying this release before using the above APIs on the client.
[#20997]
</li>

<li>
New service parameters have been added to control the backlog
associated with sockets created by NoSQL Database. These are
controllable for the Rep Node and Storage Nodes' Monitor, Admin, and
Registry Handler interfaces. The parameters
are <code>rnRHSOBacklog</code> (default
1024), <code>rnMonitorSOBacklog</code> (default 0), rnAdminSOBacklog
(default 0), <code>rnAdminSOBacklog</code> (default 0),
<code>snAdminSOBacklog</code> (default
0), <code>snMonitorSOBacklog</code> (default 0), and
<code>snRegistrySOBacklog</code> (default 1024).
[#21322]
</li>

<li>
  Previously, calling <code>Key.isPrefix</code> with an argument containing
  a smaller major or minor path than the target Key object caused an
  IndexOutOfBoundsException in certain cases.  This has been fixed.
</li>
<li>
The <code>KeyRange()</code> constructor now checks that the start
<code>Key</code> is less than the end <code>Key</code> if both are specified,
otherwise an <code>IllegalArgumentException</code> is thrown.
<code>KeyRange</code> also has <code>toString()</code> and
<code>fromString()</code> methods for encoding and decoding
<code>KeyRange</code> instances, similar to the same methods in
<code>Key</code>. [#21470]
</li>
</ol>
<!-- ========================================================= -->
<h3><a name="Utilities2023">Utility Changes:</a></h3>
<!-- ========================================================= -->
<ol>
<li>
Many new commands have been added to the CLI. See
<a href="AdminGuide/cli_command_reference.html">
    Appendix A - Command Line Interface (CLI) Command Reference
</a> of the <i>Administrator's Guide</i>
for details.
<li>
The Admin Console is now for monitoring only.
<li>
Administration CLI commands have been changed so that component ids
match the ids used in the topology display. Previously Datacenters,
Storage Nodes, Admin instances and Replication Nodes were identified
only by number. For example, the syntax to add Storage Node 17 to a
Storage Node pool, or to show the parameters for a given Replication Node was:
<blockquote>
<pre>
joinPool myStorageNodePool 17
show repnode-params 5,3
</pre>
</blockquote>
Datacenters can now be expressed as # or dc#<br>
Admin instances can now be expressed as # or admin#<br>
Storage Nodes can now be expressed as # or sn#<br>
Replication Nodes can now be expressed as groupNum,nodeNum, or rgX-rnY<br>
<p>
The commands shown above are still valid, but can also be expressed as:
<blockquote>
<pre>
joinPool myStorageNodePool sn17
show repnode-params rg5-rn3
</pre>
</blockquote>
[#21099]
</li>
</ol>

<!-- ========================================================= -->
<h3><a name="IandI2023">Documentation, Installation and
    Integration:</a></h3>
<!-- ========================================================= -->

<ol>
<li>
 The javadoc for the <code>Key.createKey</code> methods has been improved to
 warn that List instances passed as parameters are owned by the Key object
 after calling the method.  To avoid unpredictable results, they must not be
 modified.  [#20530]
</li><br>
</ol>

<!-- ============================================================ -->
<hr>
<h3 align="center"><u>Changes in 11gR2.1.2.123</u></h3>

<!-- ========================================================= -->
<h3><a name="BugFixes12">Bug fixes:</a></h3>
<!-- ========================================================= -->
<ol>
<li>
  Previously, executing a change-repnode-params plan in order to
  change Replication Node parameters for a node other than the one
  running the Admin service would fail. This operation will now
  succeed. [#20901]
</li><br>
<li>
  A deploy-storage-node plan which ran into problems when attempting
  to deploy a new storage node would leave the problematic SN in the
  store. This would require that the user either take manual action to
  remove the bad SN, or fix the problem and retry the plan. For
  convenience, the deploy-storage-node plan will now clean up if it
  runs into errors, and will not leave the failed SN behind. [#20530]
</li><br>
</ol>

<!-- ========================================================= -->
<h3><a name="General12">Performance and other General Changes:</a></h3>
<!-- ========================================================= -->
<ol>
<li>
  The command line interface's <code>snapshot create</code> command
  has been made significantly faster. Previously, it could take
  minutes if executed on a store with a large amount of data.  This
  should be reduced to seconds. [#20772]
</li>

</ol>

<!-- ========================================================= -->
<h3><a name="Utilities12">Utility Changes:</a></h3>
<!-- ========================================================= -->
<ol>
<li>
The two scripts for starting kvlite and executing control commands,
<code>bin/run-kvlite.sh</code> and <code>bin/kvctl</code>, have been replaced
by a <code>java -jar lib/kvstore-M.N.P.jar</code> command.  This provides portability
to all Java platforms, including Windows.  The two scripts are deprecated, but
will be supported for at least one release cycle.
<p>
The translation from the old script commands to the new -jar commands is as
follows:</p>
<table border="1">
    <tr><th>Old script command</th><th>New -jar command</th></tr>
    <tr>
        <td><code>bin/run-kvlite.sh args...</code></td>
        <td><code>java -jar lib/kvstore-M.N.P.jar kvlite args...</code></td>
    </tr>
    <tr>
        <td><code>bin/kvctl command args...</code></td>
        <td><code>java -jar lib/kvstore-M.N.P.jar command args...</code></td>
    </tr>
</table>
<p>
There are a few differences to be aware of between the old and new commands.
<ul>
    <li><p><code>nohup</code>, if desired, must be explicitly specified.  In
    the <code>bin/kvctl</code> script, nohup was added automatically for the
    <code>start</code> and <code>restart</code> commands.  To specify the
    equivalent command, use:</p>
    <p><code>nohup java -jar lib/kvstore-M.N.P.jar start args... &gt; /dev/null &lt;
    /dev/null 2&lt;&amp;1 &amp;</code></p>
    </li>
    <li><p>The logging configuration file for kvlite is now specified using
    standard Java syntax.  Previously, the
    <code>examples/logging.properties</code> configuration file was added
    automatically when passing <code>-logging</code> to the
    <code>run-kvlite.sh</code> script.  The new equivalent is:</p>
    <p><code>java -Djava.util.logging.config.file=examples/logging.properties
    -jar lib/kvstore-M.N.P.jar kvlite args...</code></p>
    </li>
    <li><p>Previously, the <code>-host</code> argument defaulted to
    the local machine name (via the <code>`hostname`</code> command) when
    running the <code>kvctl</code> script.  Now, for all control commands, no
    default hostname is used and the <code>-host</code> argument must be
    specified explicitly.  This change was made for two reasons: 1)
    consistency, since the port and other arguments have no default value for
    control commands, and 2) safety, since specifying an explicit hostname
    guards against accidental errors.</p>
    </li>
    <li><p>Previously, the <code>-host</code> argument defaulted to
    <code>localhost</code> when running the <code>run-kvlite.sh</code> script.
    Now, the default is the local machine name rather than (literally)
    <code>localhost</code>.  Note that the kvlite command, unlike the control
    commands, has default values for all arguments.  This is because the kvlite
    command is designed for ease-of-use during development on a single machine.
    kvlite should not be used in production or for performance testing.</p>
    </li>
    <li><p>Previously, running <code>java -jar lib/kvstore-M.N.P.jar</code>, with or
    without arguments, printed the product version.  Now, if no arguments are
    specified, a usage message is printed.  To print the version, use the
    version command:</p>
    <p><code>java -jar lib/kvstore-M.N.P.jar version</code></p>
    </li>
</ul>
</li><br>
</ol>

</body>
</html>
